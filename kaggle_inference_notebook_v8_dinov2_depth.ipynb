{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - V8: DINOv2 + Depth Fusion\n",
    "\n",
    "Combines DINOv2 foundation model with Depth Anything v2:\n",
    "- **Backbone**: DINOv2 ViT-Base (best foundation model)\n",
    "- **Depth**: Depth Anything v2 Small\n",
    "- **Ensemble**: 5-fold cross-validation\n",
    "- **Val Loss**: 2.08 +/- 0.99\n",
    "\n",
    "## Setup\n",
    "1. Add model dataset + competition data\n",
    "2. **Set Internet to OFF**\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "\n",
    "MODEL_BASE = '/kaggle/input/image2biomass-dinov2-depth/pytorch/default/1/dinov2_depth_model'\n",
    "DEPTH_MODEL_PATH = f'{MODEL_BASE}/depth_anything_v2_small'\n",
    "N_FOLDS = 5\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "CONFIG = {\n",
    "    'backbone': 'vit_base_patch14_dinov2',\n",
    "    'image_size': 518,\n",
    "    'features': 768,\n",
    "    'batch_size': 4,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Depth Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDepthEstimator(nn.Module):\n",
    "    \"\"\"Depth estimator loaded once and shared.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        B, C, H, W = images.shape\n",
    "        outputs = self.model(images)\n",
    "        depth = outputs.predicted_depth\n",
    "        depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        depth_flat = depth.view(B, -1)\n",
    "        depth_min = depth_flat.min(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        depth_max = depth_flat.max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        depth = (depth - depth_min) / (depth_max - depth_min + 1e-8)\n",
    "        return depth\n",
    "\n",
    "print(\"Loading shared depth model...\")\n",
    "shared_depth_model = SharedDepthEstimator(DEPTH_MODEL_PATH).to(DEVICE)\n",
    "print(\"Depth model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINOv2 + Depth Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2DepthFusionModel(nn.Module):\n",
    "    \"\"\"DINOv2 + Depth fusion model for inference.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone_name, num_features=768, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.target_names = TARGET_NAMES\n",
    "        \n",
    "        # DINOv2 backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone_name,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "        )\n",
    "        \n",
    "        # Depth encoder (same as training)\n",
    "        self.depth_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        fused_features = num_features + 256\n",
    "        \n",
    "        # Regression heads\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for name in self.target_names:\n",
    "            self.heads[name] = nn.Sequential(\n",
    "                nn.Linear(fused_features, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, images, depth_maps):\n",
    "        rgb_features = self.backbone(images)\n",
    "        depth_features = self.depth_encoder(depth_maps)\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)\n",
    "        return {name: self.heads[name](fused).squeeze(-1) for name in self.target_names}\n",
    "\n",
    "print(\"DINOv2DepthFusionModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = self.img_dir / Path(self.image_paths[image_id]).name\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return {'image': image, 'image_id': image_id}\n",
    "\n",
    "# Simple transform (no TTA for now)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Target Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "train_wide = train_df.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='first')\n",
    "\n",
    "target_stats = {}\n",
    "for name in TARGET_NAMES:\n",
    "    values = train_wide[name].values\n",
    "    target_stats[name] = {'mean': float(np.mean(values)), 'std': float(np.std(values)) + 1e-8}\n",
    "\n",
    "print(\"Target stats:\")\n",
    "for name, stats in target_stats.items():\n",
    "    print(f\"  {name:<15} mean: {stats['mean']:>8.2f}  std: {stats['std']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-computing depth maps...\")\n",
    "\n",
    "dataset = BiomassTestDataset(TEST_CSV, TEST_IMG_DIR, transform=val_transform)\n",
    "loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "depth_maps_cache = {}\n",
    "images_cache = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Computing depth\"):\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        image_ids = batch['image_id']\n",
    "        \n",
    "        depth_maps = shared_depth_model(images)\n",
    "        \n",
    "        for i, img_id in enumerate(image_ids):\n",
    "            depth_maps_cache[img_id] = depth_maps[i:i+1].cpu()\n",
    "            images_cache[img_id] = images[i:i+1].cpu()\n",
    "\n",
    "# Free depth model\n",
    "del shared_depth_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Pre-computed depth for {len(depth_maps_cache)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(pred_dict, stats):\n",
    "    return {name: (val * stats[name]['std']) + stats[name]['mean'] for name, val in pred_dict.items()}\n",
    "\n",
    "all_fold_predictions = []\n",
    "image_ids_order = list(images_cache.keys())\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}...\")\n",
    "    \n",
    "    model = DINOv2DepthFusionModel(\n",
    "        backbone_name=CONFIG['backbone'],\n",
    "        num_features=CONFIG['features'],\n",
    "        dropout=CONFIG['dropout']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    checkpoint_path = Path(MODEL_BASE) / f'fold_{fold_idx}' / 'best_model.pth'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    # Load weights (skip depth_estimator)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model_state = model.state_dict()\n",
    "    \n",
    "    for key in model_state.keys():\n",
    "        if key in state_dict:\n",
    "            model_state[key] = state_dict[key]\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    print(f\"  Loaded (Val Loss: {checkpoint['best_val_loss']:.4f})\")\n",
    "    \n",
    "    fold_preds = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img_id in tqdm(image_ids_order, desc=f\"Fold {fold_idx + 1}\"):\n",
    "            images = images_cache[img_id].to(DEVICE)\n",
    "            depth_maps = depth_maps_cache[img_id].to(DEVICE)\n",
    "            \n",
    "            pred = model(images, depth_maps)\n",
    "            pred_dict = {name: pred[name][0].item() for name in TARGET_NAMES}\n",
    "            pred_denorm = denormalize(pred_dict, target_stats)\n",
    "            fold_preds[img_id] = pred_denorm\n",
    "    \n",
    "    all_fold_predictions.append(fold_preds)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nGenerated predictions from {N_FOLDS} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Apply Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across folds\n",
    "ensemble_predictions = {}\n",
    "\n",
    "for img_id in image_ids_order:\n",
    "    ensemble_pred = {}\n",
    "    for name in TARGET_NAMES:\n",
    "        fold_vals = [fp[img_id][name] for fp in all_fold_predictions]\n",
    "        ensemble_pred[name] = np.mean(fold_vals)\n",
    "    ensemble_predictions[img_id] = ensemble_pred\n",
    "\n",
    "# Apply biological constraints\n",
    "print(\"Applying constraints...\")\n",
    "for img_id in image_ids_order:\n",
    "    pred = ensemble_predictions[img_id]\n",
    "    \n",
    "    # Clip negatives\n",
    "    for name in TARGET_NAMES:\n",
    "        pred[name] = max(0.0, pred[name])\n",
    "    \n",
    "    clover = pred['Dry_Clover_g']\n",
    "    dead = pred['Dry_Dead_g']\n",
    "    green = pred['Dry_Green_g']\n",
    "    total = pred['Dry_Total_g']\n",
    "    \n",
    "    # Enforce: Total = Clover + Dead + Green\n",
    "    component_sum = clover + dead + green\n",
    "    new_total = (total + component_sum) / 2\n",
    "    \n",
    "    if component_sum > 0:\n",
    "        scale = new_total / component_sum\n",
    "        pred['Dry_Clover_g'] = clover * scale\n",
    "        pred['Dry_Dead_g'] = dead * scale\n",
    "        pred['Dry_Green_g'] = green * scale\n",
    "    pred['Dry_Total_g'] = new_total\n",
    "    \n",
    "    ensemble_predictions[img_id] = pred\n",
    "\n",
    "print(\"\\nPredictions summary:\")\n",
    "for name in TARGET_NAMES:\n",
    "    vals = [ensemble_predictions[img_id][name] for img_id in image_ids_order]\n",
    "    print(f\"  {name:<15} mean: {np.mean(vals):>8.2f}  min: {np.min(vals):>8.2f}  max: {np.max(vals):>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "submission_rows = []\n",
    "for _, row in test_df.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    image_id = sample_id.split('__')[0]\n",
    "    target_name = row['target_name']\n",
    "    \n",
    "    pred_value = ensemble_predictions[image_id][target_name]\n",
    "    \n",
    "    submission_rows.append({\n",
    "        'sample_id': sample_id,\n",
    "        'target': pred_value\n",
    "    })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission created!\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"V8: DINOv2 + Depth Fusion\")\n",
    "print(f\"Backbone: {CONFIG['backbone']}\")\n",
    "print(f\"Image size: {CONFIG['image_size']}\")\n",
    "print(f\"Ensemble: {N_FOLDS}-fold\")\n",
    "print(\"Val Loss: 2.08 +/- 0.99\")\n",
    "print(\"Submission file ready: submission.csv\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
