{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - V6: Ensemble (V4 Baseline + V5 Depth Fusion)\n",
    "\n",
    "Combines two diverse models:\n",
    "- **V4**: EfficientNetV2-M baseline (scored 0.50)\n",
    "- **V5**: RGB+Depth Fusion with external data (scored 0.57)\n",
    "\n",
    "Optimized for speed with shared depth model and pre-computed depth maps.\n",
    "\n",
    "## Setup\n",
    "1. Add model dataset (ensemble-v4-v5) + competition data\n",
    "2. **Set Internet to OFF**\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import AutoModelForDepthEstimation\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "\n",
    "# Model paths\n",
    "MODEL_BASE = '/kaggle/input/ensemble-v4-v5/pytorch/default/1'\n",
    "V4_PATH = f'{MODEL_BASE}/v4_baseline'\n",
    "V5_PATH = f'{MODEL_BASE}/v5_depth_fusion'\n",
    "DEPTH_MODEL_PATH = f'{MODEL_BASE}/depth_anything_v2_small'\n",
    "\n",
    "N_FOLDS = 5\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "# Ensemble weights (can tune these)\n",
    "V4_WEIGHT = 0.4  # Baseline\n",
    "V5_WEIGHT = 0.6  # Depth fusion (scored higher)\n",
    "\n",
    "# Config\n",
    "IMAGE_SIZE = 384  # Use 384 for depth model compatibility\n",
    "BATCH_SIZE = 4\n",
    "USE_TTA = True\n",
    "TTA_ROTATIONS = False  # Skip rotations to save time\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Ensemble weights: V4={V4_WEIGHT}, V5={V5_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class V4BaselineModel(nn.Module):\n",
    "    \"\"\"V4: EfficientNetV2-M baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='tf_efficientnetv2_m', dropout=0.5, head_hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0, global_pool='avg')\n",
    "        backbone_features = self.backbone.num_features\n",
    "        \n",
    "        self.heads = nn.ModuleDict({\n",
    "            name: nn.Sequential(\n",
    "                nn.Linear(backbone_features, head_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(head_hidden_dim, 1)\n",
    "            ) for name in TARGET_NAMES\n",
    "        })\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return {name: self.heads[name](features).squeeze(-1) for name in TARGET_NAMES}\n",
    "\n",
    "\n",
    "class SharedDepthEstimator(nn.Module):\n",
    "    \"\"\"Depth estimator loaded once.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        B, C, H, W = images.shape\n",
    "        outputs = self.model(images)\n",
    "        depth = outputs.predicted_depth\n",
    "        depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        depth_flat = depth.view(B, -1)\n",
    "        depth_min = depth_flat.min(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        depth_max = depth_flat.max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        return (depth - depth_min) / (depth_max - depth_min + 1e-8)\n",
    "\n",
    "\n",
    "class V5DepthFusionModel(nn.Module):\n",
    "    \"\"\"V5: Lightweight fusion model (uses pre-computed depth).\"\"\"\n",
    "    \n",
    "    def __init__(self, rgb_backbone='efficientnetv2_rw_m', dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.rgb_encoder = timm.create_model(rgb_backbone, pretrained=False, num_classes=0, global_pool='avg')\n",
    "        rgb_features = self.rgb_encoder.num_features\n",
    "        \n",
    "        self.depth_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        fused_features = rgb_features + 256\n",
    "        self.heads = nn.ModuleDict({\n",
    "            name: nn.Sequential(\n",
    "                nn.Linear(fused_features, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(64, 1)\n",
    "            ) for name in TARGET_NAMES\n",
    "        })\n",
    "    \n",
    "    def forward(self, images, depth_maps):\n",
    "        rgb_features = self.rgb_encoder(images)\n",
    "        depth_features = self.depth_encoder(depth_maps)\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)\n",
    "        return {name: self.heads[name](fused).squeeze(-1) for name in TARGET_NAMES}\n",
    "\n",
    "print(\"Models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = self.img_dir / Path(self.image_paths[image_id]).name\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return {'image': image, 'image_id': image_id}\n",
    "\n",
    "def get_tta_transforms(image_size, include_rotations=False):\n",
    "    transforms = []\n",
    "    flip_configs = [(False, False), (True, False), (False, True), (True, True)]\n",
    "    rotation_angles = [0, 90] if include_rotations else [0]\n",
    "    \n",
    "    for hflip, vflip in flip_configs:\n",
    "        for angle in rotation_angles:\n",
    "            aug_list = [A.Resize(image_size, image_size)]\n",
    "            if hflip:\n",
    "                aug_list.append(A.HorizontalFlip(p=1.0))\n",
    "            if vflip:\n",
    "                aug_list.append(A.VerticalFlip(p=1.0))\n",
    "            if angle != 0:\n",
    "                aug_list.append(A.Rotate(limit=(angle, angle), p=1.0, border_mode=0))\n",
    "            aug_list.extend([A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD), ToTensorV2()])\n",
    "            transforms.append(A.Compose(aug_list))\n",
    "    return transforms\n",
    "\n",
    "tta_transforms = get_tta_transforms(IMAGE_SIZE, include_rotations=TTA_ROTATIONS)\n",
    "print(f\"Using {len(tta_transforms)} TTA transforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Target Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "train_wide = train_df.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='first')\n",
    "\n",
    "target_stats = {}\n",
    "for name in TARGET_NAMES:\n",
    "    values = train_wide[name].values\n",
    "    target_stats[name] = {'mean': float(np.mean(values)), 'std': float(np.std(values)) + 1e-8}\n",
    "\n",
    "def denormalize(pred_dict, stats):\n",
    "    return {name: (val * stats[name]['std']) + stats[name]['mean'] for name, val in pred_dict.items()}\n",
    "\n",
    "print(\"Target stats loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute Data for All TTA Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load depth model once\n",
    "print(\"Loading shared depth model...\")\n",
    "depth_model = SharedDepthEstimator(DEPTH_MODEL_PATH).to(DEVICE)\n",
    "\n",
    "# Pre-compute depth maps for all TTA transforms\n",
    "print(\"\\nPre-computing images and depth maps...\")\n",
    "all_data = {}  # {tta_idx: {image_id: {'image': tensor, 'depth': tensor}}}\n",
    "\n",
    "for tta_idx, transform in enumerate(tta_transforms):\n",
    "    print(f\"  TTA {tta_idx + 1}/{len(tta_transforms)}...\")\n",
    "    \n",
    "    dataset = BiomassTestDataset(TEST_CSV, TEST_IMG_DIR, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    tta_data = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['image'].to(DEVICE)\n",
    "            image_ids = batch['image_id']\n",
    "            depth_maps = depth_model(images)\n",
    "            \n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                tta_data[img_id] = {\n",
    "                    'image': images[i:i+1].cpu(),\n",
    "                    'depth': depth_maps[i:i+1].cpu()\n",
    "                }\n",
    "    \n",
    "    all_data[tta_idx] = tta_data\n",
    "\n",
    "# Free depth model\n",
    "del depth_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "image_ids_order = list(all_data[0].keys())\n",
    "print(f\"\\nPre-computed data for {len(image_ids_order)} images x {len(tta_transforms)} TTA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate V4 Predictions (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"V4: Baseline EfficientNetV2-M\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "v4_predictions = {name: {img_id: 0.0 for img_id in image_ids_order} for name in TARGET_NAMES}\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = V4BaselineModel().to(DEVICE)\n",
    "    checkpoint = torch.load(f'{V4_PATH}/fold_{fold_idx}/best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"  Val Loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        for tta_idx in range(len(tta_transforms)):\n",
    "            for img_id in image_ids_order:\n",
    "                images = all_data[tta_idx][img_id]['image'].to(DEVICE)\n",
    "                pred = model(images)\n",
    "                pred_denorm = denormalize({n: pred[n][0].item() for n in TARGET_NAMES}, target_stats)\n",
    "                \n",
    "                for name in TARGET_NAMES:\n",
    "                    v4_predictions[name][img_id] += pred_denorm[name]\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Average across folds and TTA\n",
    "n_total = N_FOLDS * len(tta_transforms)\n",
    "for name in TARGET_NAMES:\n",
    "    for img_id in image_ids_order:\n",
    "        v4_predictions[name][img_id] /= n_total\n",
    "\n",
    "print(\"\\nV4 predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate V5 Predictions (Depth Fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"V5: RGB+Depth Fusion\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "v5_predictions = {name: {img_id: 0.0 for img_id in image_ids_order} for name in TARGET_NAMES}\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = V5DepthFusionModel().to(DEVICE)\n",
    "    checkpoint = torch.load(f'{V5_PATH}/fold_{fold_idx}/best_model.pth', map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    # Load weights (skip depth_estimator)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model_state = model.state_dict()\n",
    "    for key in model_state.keys():\n",
    "        if key in state_dict:\n",
    "            model_state[key] = state_dict[key]\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    print(f\"  Val Loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        for tta_idx in range(len(tta_transforms)):\n",
    "            for img_id in image_ids_order:\n",
    "                images = all_data[tta_idx][img_id]['image'].to(DEVICE)\n",
    "                depth_maps = all_data[tta_idx][img_id]['depth'].to(DEVICE)\n",
    "                pred = model(images, depth_maps)\n",
    "                pred_denorm = denormalize({n: pred[n][0].item() for n in TARGET_NAMES}, target_stats)\n",
    "                \n",
    "                for name in TARGET_NAMES:\n",
    "                    v5_predictions[name][img_id] += pred_denorm[name]\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Average across folds and TTA\n",
    "for name in TARGET_NAMES:\n",
    "    for img_id in image_ids_order:\n",
    "        v5_predictions[name][img_id] /= n_total\n",
    "\n",
    "print(\"\\nV5 predictions complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(f\"Ensembling: V4 x {V4_WEIGHT} + V5 x {V5_WEIGHT}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ensemble_predictions = {name: {} for name in TARGET_NAMES}\n",
    "\n",
    "for img_id in image_ids_order:\n",
    "    for name in TARGET_NAMES:\n",
    "        v4_pred = v4_predictions[name][img_id]\n",
    "        v5_pred = v5_predictions[name][img_id]\n",
    "        ensemble_predictions[name][img_id] = V4_WEIGHT * v4_pred + V5_WEIGHT * v5_pred\n",
    "\n",
    "# Apply biological constraints\n",
    "print(\"\\nApplying biological constraints...\")\n",
    "for img_id in image_ids_order:\n",
    "    # Clip negatives\n",
    "    for name in TARGET_NAMES:\n",
    "        ensemble_predictions[name][img_id] = max(0.0, ensemble_predictions[name][img_id])\n",
    "    \n",
    "    clover = ensemble_predictions['Dry_Clover_g'][img_id]\n",
    "    dead = ensemble_predictions['Dry_Dead_g'][img_id]\n",
    "    green = ensemble_predictions['Dry_Green_g'][img_id]\n",
    "    gdm = ensemble_predictions['GDM_g'][img_id]\n",
    "    total = ensemble_predictions['Dry_Total_g'][img_id]\n",
    "    \n",
    "    # GDM = Green + Clover\n",
    "    gdm_calc = green + clover\n",
    "    adj_gdm = (gdm + gdm_calc) / 2\n",
    "    if gdm_calc > 0:\n",
    "        scale = adj_gdm / gdm_calc\n",
    "        ensemble_predictions['Dry_Green_g'][img_id] = green * scale\n",
    "        ensemble_predictions['Dry_Clover_g'][img_id] = clover * scale\n",
    "    ensemble_predictions['GDM_g'][img_id] = adj_gdm\n",
    "    \n",
    "    # Total = GDM + Dead\n",
    "    total_calc = adj_gdm + dead\n",
    "    adj_total = (total + total_calc) / 2\n",
    "    if adj_total > adj_gdm:\n",
    "        ensemble_predictions['Dry_Dead_g'][img_id] = adj_total - adj_gdm\n",
    "    else:\n",
    "        ensemble_predictions['Dry_Dead_g'][img_id] = 0.0\n",
    "        adj_total = adj_gdm\n",
    "    ensemble_predictions['Dry_Total_g'][img_id] = adj_total\n",
    "\n",
    "print(\"\\nEnsemble predictions summary:\")\n",
    "for name in TARGET_NAMES:\n",
    "    vals = list(ensemble_predictions[name].values())\n",
    "    print(f\"  {name:<15} mean: {np.mean(vals):>8.2f}\")\n",
    "\n",
    "print(\"\\nIndividual model predictions (for comparison):\")\n",
    "for name in TARGET_NAMES:\n",
    "    v4_vals = list(v4_predictions[name].values())\n",
    "    v5_vals = list(v5_predictions[name].values())\n",
    "    ens_vals = list(ensemble_predictions[name].values())\n",
    "    print(f\"  {name:<15} V4: {np.mean(v4_vals):>7.2f}  V5: {np.mean(v5_vals):>7.2f}  Ens: {np.mean(ens_vals):>7.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rows = []\n",
    "for img_id in image_ids_order:\n",
    "    for name in TARGET_NAMES:\n",
    "        submission_rows.append({\n",
    "            'sample_id': f\"{img_id}__{name}\",\n",
    "            'target': ensemble_predictions[name][img_id]\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Submission created: submission.csv\")\n",
    "print(f\"V6 Ensemble: V4 ({V4_WEIGHT}) + V5 ({V5_WEIGHT})\")\n",
    "print(\"=\"*60)\n",
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
