{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56139797",
   "metadata": {
    "papermill": {
     "duration": 0.004183,
     "end_time": "2026-01-11T03:11:26.432220",
     "exception": false,
     "start_time": "2026-01-11T03:11:26.428037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CSIRO Image2Biomass - Optimized ResNet50 Inference\n",
    "\n",
    "This notebook generates predictions using an optimized ResNet50 model trained with:\n",
    "- Huber loss (delta=2.0)\n",
    "- Aggressive augmentation\n",
    "- AdamW optimizer\n",
    "- Validation loss: 0.4387 (69% improvement over baseline)\n",
    "\n",
    "## Setup Instructions\n",
    "1. Upload the model checkpoint as a Kaggle Dataset\n",
    "2. Add the competition data\n",
    "3. Run all cells to generate submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6945a91",
   "metadata": {
    "papermill": {
     "duration": 0.002515,
     "end_time": "2026-01-11T03:11:26.437409",
     "exception": false,
     "start_time": "2026-01-11T03:11:26.434894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa9711d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:26.444352Z",
     "iopub.status.busy": "2026-01-11T03:11:26.443632Z",
     "iopub.status.idle": "2026-01-11T03:11:49.699140Z",
     "shell.execute_reply": "2026-01-11T03:11:49.698413Z"
    },
    "papermill": {
     "duration": 23.261087,
     "end_time": "2026-01-11T03:11:49.700888",
     "exception": false,
     "start_time": "2026-01-11T03:11:26.439801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010315e9",
   "metadata": {
    "papermill": {
     "duration": 0.002509,
     "end_time": "2026-01-11T03:11:49.706526",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.704017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bdcf5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.713451Z",
     "iopub.status.busy": "2026-01-11T03:11:49.713122Z",
     "iopub.status.idle": "2026-01-11T03:11:49.722148Z",
     "shell.execute_reply": "2026-01-11T03:11:49.719783Z"
    },
    "papermill": {
     "duration": 0.016166,
     "end_time": "2026-01-11T03:11:49.725190",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.709024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "TRAIN_IMG_DIR = '/kaggle/input/csiro-biomass/train'\n",
    "\n",
    "# Model checkpoint\n",
    "CHECKPOINT_PATH = '/kaggle/input/biomass-resnet50-optimized/pytorch/default/1/best_model.pth'\n",
    "\n",
    "# Target names\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "# Model config (from Optuna optimization)\n",
    "CONFIG = {\n",
    "    'backbone': 'resnet50',\n",
    "    'pretrained': True,\n",
    "    'dropout': 0.1,\n",
    "    'head_hidden_dim': 256,\n",
    "    'image_size': 384,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 2\n",
    "}\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79863a0d",
   "metadata": {
    "papermill": {
     "duration": 0.002609,
     "end_time": "2026-01-11T03:11:49.730988",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.728379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8547d83c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.738181Z",
     "iopub.status.busy": "2026-01-11T03:11:49.737504Z",
     "iopub.status.idle": "2026-01-11T03:11:49.745916Z",
     "shell.execute_reply": "2026-01-11T03:11:49.744642Z"
    },
    "papermill": {
     "duration": 0.014307,
     "end_time": "2026-01-11T03:11:49.747748",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.733441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskResNet(nn.Module):\n",
    "    \"\"\"Multi-task ResNet for biomass prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='resnet50', num_targets=5, pretrained=True,\n",
    "                 dropout=0.3, head_hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained backbone\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained, num_classes=0)\n",
    "        backbone_features = self.backbone.num_features\n",
    "        \n",
    "        # Create prediction heads\n",
    "        self.heads = nn.ModuleDict({\n",
    "            target_name: self._make_head(backbone_features, head_hidden_dim, dropout)\n",
    "            for target_name in TARGET_NAMES\n",
    "        })\n",
    "    \n",
    "    def _make_head(self, in_features, hidden_dim, dropout):\n",
    "        \"\"\"Create a prediction head.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        outputs = {\n",
    "            target_name: self.heads[target_name](features).squeeze(-1)\n",
    "            for target_name in TARGET_NAMES\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38da152a",
   "metadata": {
    "papermill": {
     "duration": 0.002545,
     "end_time": "2026-01-11T03:11:49.754198",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.751653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b68a59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.761068Z",
     "iopub.status.busy": "2026-01-11T03:11:49.760742Z",
     "iopub.status.idle": "2026-01-11T03:11:49.769166Z",
     "shell.execute_reply": "2026-01-11T03:11:49.767601Z"
    },
    "papermill": {
     "duration": 0.014047,
     "end_time": "2026-01-11T03:11:49.770776",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.756729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined\n"
     ]
    }
   ],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    \"\"\"Test dataset for biomass prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, img_dir, transform=None, target_stats=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.target_stats = target_stats\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_filename = Path(self.image_paths[image_id]).name\n",
    "        image_path = self.img_dir / image_filename\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4d24e",
   "metadata": {
    "papermill": {
     "duration": 0.002545,
     "end_time": "2026-01-11T03:11:49.776279",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.773734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Get Target Statistics from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "186140d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.783819Z",
     "iopub.status.busy": "2026-01-11T03:11:49.783368Z",
     "iopub.status.idle": "2026-01-11T03:11:49.889392Z",
     "shell.execute_reply": "2026-01-11T03:11:49.888088Z"
    },
    "papermill": {
     "duration": 0.112522,
     "end_time": "2026-01-11T03:11:49.891410",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.778888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target normalization statistics:\n",
      "  Dry_Clover_g         mean:     6.65  std:    12.10\n",
      "  Dry_Dead_g           mean:    12.04  std:    12.38\n",
      "  Dry_Green_g          mean:    26.62  std:    25.37\n",
      "  Dry_Total_g          mean:    45.32  std:    27.94\n",
      "  GDM_g                mean:    33.27  std:    24.90\n"
     ]
    }
   ],
   "source": [
    "# Load training data to compute normalization statistics\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "\n",
    "# Pivot to wide format\n",
    "train_wide = train_df.pivot_table(\n",
    "    index='image_id',\n",
    "    columns='target_name',\n",
    "    values='target',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Compute statistics\n",
    "target_stats = {}\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = train_wide[target_name].values\n",
    "    target_stats[target_name] = {\n",
    "        'mean': float(np.mean(values)),\n",
    "        'std': float(np.std(values)) + 1e-8\n",
    "    }\n",
    "\n",
    "print(\"Target normalization statistics:\")\n",
    "for target_name, stats in target_stats.items():\n",
    "    print(f\"  {target_name:<20} mean: {stats['mean']:>8.2f}  std: {stats['std']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d39ba",
   "metadata": {
    "papermill": {
     "duration": 0.002607,
     "end_time": "2026-01-11T03:11:49.897056",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.894449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Create Test Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294b450c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.904219Z",
     "iopub.status.busy": "2026-01-11T03:11:49.903914Z",
     "iopub.status.idle": "2026-01-11T03:11:49.923382Z",
     "shell.execute_reply": "2026-01-11T03:11:49.922269Z"
    },
    "papermill": {
     "duration": 0.025827,
     "end_time": "2026-01-11T03:11:49.925543",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.899716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: 1 samples\n",
      "Test batches: 1\n"
     ]
    }
   ],
   "source": [
    "# Define validation transforms\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = BiomassTestDataset(\n",
    "    csv_path=TEST_CSV,\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=val_transform,\n",
    "    target_stats=target_stats\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64551c4a",
   "metadata": {
    "papermill": {
     "duration": 0.002825,
     "end_time": "2026-01-11T03:11:49.931535",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.928710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Load Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c55af24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:49.938337Z",
     "iopub.status.busy": "2026-01-11T03:11:49.938052Z",
     "iopub.status.idle": "2026-01-11T03:11:53.617184Z",
     "shell.execute_reply": "2026-01-11T03:11:53.615946Z"
    },
    "papermill": {
     "duration": 3.68516,
     "end_time": "2026-01-11T03:11:53.619331",
     "exception": false,
     "start_time": "2026-01-11T03:11:49.934171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model: resnet50...\n",
      "Loading checkpoint from /kaggle/input/biomass-resnet50-optimized/pytorch/default/1/best_model.pth...\n",
      "Checkpoint loaded successfully!\n",
      "  Epoch: 48\n",
      "  Best Val Loss: 0.4387\n",
      "Total parameters: 26,132,037\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(f\"Creating model: {CONFIG['backbone']}...\")\n",
    "model = MultiTaskResNet(\n",
    "    backbone=CONFIG['backbone'],\n",
    "    num_targets=len(TARGET_NAMES),\n",
    "    pretrained=False,  # We'll load trained weights\n",
    "    dropout=CONFIG['dropout'],\n",
    "    head_hidden_dim=CONFIG['head_hidden_dim']\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Checkpoint loaded successfully!\")\n",
    "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "print(f\"  Best Val Loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ebf98",
   "metadata": {
    "papermill": {
     "duration": 0.005079,
     "end_time": "2026-01-11T03:11:53.627475",
     "exception": false,
     "start_time": "2026-01-11T03:11:53.622396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2375c576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:53.638451Z",
     "iopub.status.busy": "2026-01-11T03:11:53.637723Z",
     "iopub.status.idle": "2026-01-11T03:11:54.385055Z",
     "shell.execute_reply": "2026-01-11T03:11:54.383899Z"
    },
    "papermill": {
     "duration": 0.755567,
     "end_time": "2026-01-11T03:11:54.386944",
     "exception": false,
     "start_time": "2026-01-11T03:11:53.631377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating predictions:   0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Generating predictions: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated predictions for 1 images\n",
      "Applying constraint enforcement...\n",
      "Constraint violations:\n",
      "  Mean: 0.000000g\n",
      "  Max: 0.000000g\n",
      "  All exact: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def denormalize_predictions(pred_dict, target_stats):\n",
    "    \"\"\"Denormalize predictions back to original scale.\"\"\"\n",
    "    denormalized = {}\n",
    "    for target_name, value in pred_dict.items():\n",
    "        stats = target_stats[target_name]\n",
    "        denormalized[target_name] = (value * stats['std']) + stats['mean']\n",
    "    return denormalized\n",
    "\n",
    "def enforce_constraint(predictions, method='average'):\n",
    "    \"\"\"Enforce constraint: Dry_Total = Dry_Clover + Dry_Dead + Dry_Green\"\"\"\n",
    "    enforced = {}\n",
    "    \n",
    "    for image_id, pred_dict in predictions.items():\n",
    "        pred = pred_dict.copy()\n",
    "        \n",
    "        clover = pred['Dry_Clover_g']\n",
    "        dead = pred['Dry_Dead_g']\n",
    "        green = pred['Dry_Green_g']\n",
    "        total = pred['Dry_Total_g']\n",
    "        \n",
    "        component_sum = clover + dead + green\n",
    "        \n",
    "        if method == 'average':\n",
    "            # Average the predicted total and sum of components\n",
    "            new_total = (total + component_sum) / 2\n",
    "            \n",
    "            # Distribute discrepancy proportionally\n",
    "            if component_sum > 0:\n",
    "                scale = new_total / component_sum\n",
    "                pred['Dry_Clover_g'] = clover * scale\n",
    "                pred['Dry_Dead_g'] = dead * scale\n",
    "                pred['Dry_Green_g'] = green * scale\n",
    "                pred['Dry_Total_g'] = new_total\n",
    "            else:\n",
    "                pred['Dry_Total_g'] = 0.0\n",
    "        \n",
    "        enforced[image_id] = pred\n",
    "    \n",
    "    return enforced\n",
    "\n",
    "# Run inference\n",
    "print(\"Running inference...\")\n",
    "predictions = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating predictions\"):\n",
    "        images = batch['image'].to(DEVICE)\n",
    "        image_ids = batch['image_id']\n",
    "        \n",
    "        # Get predictions\n",
    "        pred = model(images)\n",
    "        \n",
    "        # Store predictions for each image\n",
    "        for i, image_id in enumerate(image_ids):\n",
    "            pred_dict = {\n",
    "                target_name: pred[target_name][i].cpu().item()\n",
    "                for target_name in TARGET_NAMES\n",
    "            }\n",
    "            # Denormalize\n",
    "            pred_dict = denormalize_predictions(pred_dict, target_stats)\n",
    "            predictions[image_id] = pred_dict\n",
    "\n",
    "print(f\"Generated predictions for {len(predictions)} images\")\n",
    "\n",
    "# Apply constraint enforcement\n",
    "print(\"Applying constraint enforcement...\")\n",
    "predictions = enforce_constraint(predictions, method='average')\n",
    "\n",
    "# Check constraint violations\n",
    "violations = []\n",
    "for image_id, pred in predictions.items():\n",
    "    total = pred['Dry_Total_g']\n",
    "    component_sum = pred['Dry_Clover_g'] + pred['Dry_Dead_g'] + pred['Dry_Green_g']\n",
    "    violation = abs(total - component_sum)\n",
    "    violations.append(violation)\n",
    "\n",
    "print(f\"Constraint violations:\")\n",
    "print(f\"  Mean: {np.mean(violations):.6f}g\")\n",
    "print(f\"  Max: {np.max(violations):.6f}g\")\n",
    "print(f\"  All exact: {all(v < 1e-6 for v in violations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fee3a",
   "metadata": {
    "papermill": {
     "duration": 0.003171,
     "end_time": "2026-01-11T03:11:54.393776",
     "exception": false,
     "start_time": "2026-01-11T03:11:54.390605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fad28e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:54.402136Z",
     "iopub.status.busy": "2026-01-11T03:11:54.401790Z",
     "iopub.status.idle": "2026-01-11T03:11:54.432918Z",
     "shell.execute_reply": "2026-01-11T03:11:54.431354Z"
    },
    "papermill": {
     "duration": 0.038428,
     "end_time": "2026-01-11T03:11:54.435411",
     "exception": false,
     "start_time": "2026-01-11T03:11:54.396983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created!\n",
      "Shape: (5, 2)\n",
      "\n",
      "First few predictions:\n",
      "                    sample_id     target\n",
      "0  ID1001187975__Dry_Clover_g   1.158229\n",
      "1    ID1001187975__Dry_Dead_g  27.936712\n",
      "2   ID1001187975__Dry_Green_g  24.517426\n",
      "3   ID1001187975__Dry_Total_g  53.612367\n",
      "4         ID1001187975__GDM_g  25.084344\n",
      "\n",
      "Summary statistics:\n",
      "count     5.000000\n",
      "mean     26.461816\n",
      "std      18.609657\n",
      "min       1.158229\n",
      "25%      24.517426\n",
      "50%      25.084344\n",
      "75%      27.936712\n",
      "max      53.612367\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load test.csv to get correct sample_id ordering\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Create submission rows\n",
    "submission_rows = []\n",
    "for _, row in test_df.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    image_id = sample_id.split('__')[0]\n",
    "    target_name = row['target_name']\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_value = predictions[image_id][target_name]\n",
    "    \n",
    "    submission_rows.append({\n",
    "        'sample_id': sample_id,\n",
    "        'target': pred_value\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created!\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(submission_df['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843dc1cf",
   "metadata": {
    "papermill": {
     "duration": 0.003118,
     "end_time": "2026-01-11T03:11:54.442323",
     "exception": false,
     "start_time": "2026-01-11T03:11:54.439205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Display Predictions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b4ee51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-11T03:11:54.450742Z",
     "iopub.status.busy": "2026-01-11T03:11:54.450393Z",
     "iopub.status.idle": "2026-01-11T03:11:54.457488Z",
     "shell.execute_reply": "2026-01-11T03:11:54.456353Z"
    },
    "papermill": {
     "duration": 0.013537,
     "end_time": "2026-01-11T03:11:54.459041",
     "exception": false,
     "start_time": "2026-01-11T03:11:54.445504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions by target:\n",
      "  Dry_Clover_g         mean:     1.16  min:     1.16  max:     1.16\n",
      "  Dry_Dead_g           mean:    27.94  min:    27.94  max:    27.94\n",
      "  Dry_Green_g          mean:    24.52  min:    24.52  max:    24.52\n",
      "  Dry_Total_g          mean:    53.61  min:    53.61  max:    53.61\n",
      "  GDM_g                mean:    25.08  min:    25.08  max:    25.08\n",
      "\n",
      "======================================================================\n",
      "Submission file ready: submission.csv\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPredictions by target:\")\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = [pred[target_name] for pred in predictions.values()]\n",
    "    print(f\"  {target_name:<20} mean: {np.mean(values):>8.2f}  \"\n",
    "          f\"min: {np.min(values):>8.2f}  max: {np.max(values):>8.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Submission file ready: submission.csv\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14254895,
     "sourceId": 112509,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 557177,
     "modelInstanceId": 544074,
     "sourceId": 715802,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 34.900739,
   "end_time": "2026-01-11T03:11:57.595259",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-11T03:11:22.694520",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
