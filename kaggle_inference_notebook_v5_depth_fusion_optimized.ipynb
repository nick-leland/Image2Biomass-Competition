{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - V5: RGB+Depth Fusion (Optimized)\n",
    "\n",
    "**Optimized for speed:**\n",
    "- Depth model loaded ONCE and shared across all folds\n",
    "- Depth maps pre-computed for each TTA transform\n",
    "- Only RGB encoder runs per fold (much faster)\n",
    "\n",
    "## Setup\n",
    "1. Add model dataset + competition data\n",
    "2. **Set Internet to OFF**\n",
    "3. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "\n",
    "MODEL_BASE = '/kaggle/input/image2biomass-depth-fusion-model/pytorch/default/1'\n",
    "DEPTH_MODEL_PATH = f'{MODEL_BASE}/depth_anything_v2_small'\n",
    "N_FOLDS = 5\n",
    "\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "CONFIG = {\n",
    "    'backbone': 'efficientnetv2_rw_m',\n",
    "    'image_size': 384,\n",
    "    'batch_size': 4,\n",
    "    'dropout': 0.3\n",
    "}\n",
    "\n",
    "# Reduce TTA for speed (4 transforms instead of 8)\n",
    "USE_TTA = True\n",
    "TTA_ROTATIONS = False  # Skip rotations to halve inference time\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Depth Estimator (loaded once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedDepthEstimator(nn.Module):\n",
    "    \"\"\"Depth estimator loaded once and shared across all fold models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForDepthEstimation.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, images):\n",
    "        B, C, H, W = images.shape\n",
    "        outputs = self.model(images)\n",
    "        depth = outputs.predicted_depth\n",
    "        depth = F.interpolate(depth.unsqueeze(1), size=(H, W), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        depth_flat = depth.view(B, -1)\n",
    "        depth_min = depth_flat.min(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        depth_max = depth_flat.max(dim=1, keepdim=True)[0].view(B, 1, 1, 1)\n",
    "        depth = (depth - depth_min) / (depth_max - depth_min + 1e-8)\n",
    "        return depth\n",
    "\n",
    "# Load depth model ONCE\n",
    "print(\"Loading shared depth model...\")\n",
    "shared_depth_model = SharedDepthEstimator(DEPTH_MODEL_PATH).to(DEVICE)\n",
    "print(\"Depth model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightweight Fusion Model (uses pre-computed depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightweightFusionModel(nn.Module):\n",
    "    \"\"\"Fusion model that takes pre-computed depth maps (no depth estimation).\"\"\"\n",
    "    \n",
    "    def __init__(self, rgb_backbone, dropout=0.3, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.target_names = TARGET_NAMES\n",
    "        \n",
    "        # RGB Encoder\n",
    "        self.rgb_encoder = timm.create_model(\n",
    "            rgb_backbone, pretrained=pretrained, num_classes=0, global_pool='avg'\n",
    "        )\n",
    "        rgb_features = self.rgb_encoder.num_features\n",
    "        \n",
    "        # Depth Feature Encoder (same architecture as training)\n",
    "        self.depth_encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        depth_features = 256\n",
    "        fused_features = rgb_features + depth_features\n",
    "        \n",
    "        # Regression heads\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for name in self.target_names:\n",
    "            self.heads[name] = nn.Sequential(\n",
    "                nn.Linear(fused_features, 256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, images, depth_maps):\n",
    "        \"\"\"Forward with pre-computed depth maps.\"\"\"\n",
    "        rgb_features = self.rgb_encoder(images)\n",
    "        depth_features = self.depth_encoder(depth_maps)\n",
    "        fused = torch.cat([rgb_features, depth_features], dim=1)\n",
    "        \n",
    "        return {name: self.heads[name](fused).squeeze(-1) for name in self.target_names}\n",
    "\n",
    "print(\"LightweightFusionModel defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_path = self.img_dir / Path(self.image_paths[image_id]).name\n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return {'image': image, 'image_id': image_id}\n",
    "\n",
    "def get_tta_transforms(image_size=384, include_rotations=False):\n",
    "    \"\"\"Get TTA transforms - 4 without rotations, 8 with.\"\"\"\n",
    "    transforms = []\n",
    "    flip_configs = [(False, False), (True, False), (False, True), (True, True)]\n",
    "    rotation_angles = [0, 90] if include_rotations else [0]\n",
    "    \n",
    "    for hflip, vflip in flip_configs:\n",
    "        for angle in rotation_angles:\n",
    "            aug_list = [A.Resize(image_size, image_size)]\n",
    "            if hflip:\n",
    "                aug_list.append(A.HorizontalFlip(p=1.0))\n",
    "            if vflip:\n",
    "                aug_list.append(A.VerticalFlip(p=1.0))\n",
    "            if angle != 0:\n",
    "                aug_list.append(A.Rotate(limit=(angle, angle), p=1.0, border_mode=0))\n",
    "            aug_list.extend([A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD), ToTensorV2()])\n",
    "            transforms.append(A.Compose(aug_list))\n",
    "    return transforms\n",
    "\n",
    "tta_transforms = get_tta_transforms(CONFIG['image_size'], include_rotations=TTA_ROTATIONS)\n",
    "print(f\"Using {len(tta_transforms)} TTA transforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Target Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "train_wide = train_df.pivot_table(index='image_id', columns='target_name', values='target', aggfunc='first')\n",
    "\n",
    "target_stats = {}\n",
    "for name in TARGET_NAMES:\n",
    "    values = train_wide[name].values\n",
    "    target_stats[name] = {'mean': float(np.mean(values)), 'std': float(np.std(values)) + 1e-8}\n",
    "\n",
    "print(\"Target stats loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-compute Depth Maps for All TTA Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-computing depth maps for all TTA transforms...\")\n",
    "all_depth_maps = {}  # {tta_idx: {image_id: depth_tensor}}\n",
    "all_images = {}  # {tta_idx: {image_id: image_tensor}}\n",
    "\n",
    "for tta_idx, transform in enumerate(tta_transforms):\n",
    "    print(f\"  TTA {tta_idx + 1}/{len(tta_transforms)}...\")\n",
    "    \n",
    "    dataset = BiomassTestDataset(TEST_CSV, TEST_IMG_DIR, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "    \n",
    "    tta_depths = {}\n",
    "    tta_images = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['image'].to(DEVICE)\n",
    "            image_ids = batch['image_id']\n",
    "            \n",
    "            # Compute depth maps\n",
    "            depth_maps = shared_depth_model(images)\n",
    "            \n",
    "            # Store on CPU to save GPU memory\n",
    "            for i, img_id in enumerate(image_ids):\n",
    "                tta_depths[img_id] = depth_maps[i:i+1].cpu()\n",
    "                tta_images[img_id] = images[i:i+1].cpu()\n",
    "    \n",
    "    all_depth_maps[tta_idx] = tta_depths\n",
    "    all_images[tta_idx] = tta_images\n",
    "\n",
    "# Free depth model from GPU\n",
    "del shared_depth_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Pre-computed depth maps for {len(all_depth_maps[0])} images x {len(tta_transforms)} TTA transforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(pred_dict, stats):\n",
    "    return {name: (val * stats[name]['std']) + stats[name]['mean'] for name, val in pred_dict.items()}\n",
    "\n",
    "all_fold_predictions = []\n",
    "image_ids_order = list(all_images[0].keys())\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}...\")\n",
    "    \n",
    "    # Create lightweight model (no depth estimator)\n",
    "    model = LightweightFusionModel(\n",
    "        rgb_backbone=CONFIG['backbone'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        pretrained=False\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = Path(MODEL_BASE) / f'fold_{fold_idx}' / 'best_model.pth'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "    # Load only the weights we need (skip depth_estimator weights)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    model_state = model.state_dict()\n",
    "    \n",
    "    for key in model_state.keys():\n",
    "        if key in state_dict:\n",
    "            model_state[key] = state_dict[key]\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    print(f\"  Loaded (Val Loss: {checkpoint['best_val_loss']:.4f})\")\n",
    "    \n",
    "    # Generate predictions using pre-computed depth maps\n",
    "    fold_preds = {name: {img_id: 0.0 for img_id in image_ids_order} for name in TARGET_NAMES}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for tta_idx in range(len(tta_transforms)):\n",
    "            for img_id in image_ids_order:\n",
    "                images = all_images[tta_idx][img_id].to(DEVICE)\n",
    "                depth_maps = all_depth_maps[tta_idx][img_id].to(DEVICE)\n",
    "                \n",
    "                pred = model(images, depth_maps)\n",
    "                pred_denorm = denormalize({n: pred[n][0].item() for n in TARGET_NAMES}, target_stats)\n",
    "                \n",
    "                for name in TARGET_NAMES:\n",
    "                    fold_preds[name][img_id] += pred_denorm[name]\n",
    "    \n",
    "    # Average TTA\n",
    "    n_tta = len(tta_transforms)\n",
    "    for name in TARGET_NAMES:\n",
    "        for img_id in image_ids_order:\n",
    "            fold_preds[name][img_id] /= n_tta\n",
    "    \n",
    "    all_fold_predictions.append(fold_preds)\n",
    "    \n",
    "    # Free GPU memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nGenerated predictions from {N_FOLDS} folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Apply Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across folds\n",
    "ensemble_predictions = {name: {} for name in TARGET_NAMES}\n",
    "\n",
    "for img_id in image_ids_order:\n",
    "    for name in TARGET_NAMES:\n",
    "        fold_vals = [fp[name][img_id] for fp in all_fold_predictions]\n",
    "        ensemble_predictions[name][img_id] = np.mean(fold_vals)\n",
    "\n",
    "# Apply biological constraints\n",
    "print(\"Applying biological constraints...\")\n",
    "for img_id in image_ids_order:\n",
    "    # Clip negatives\n",
    "    for name in TARGET_NAMES:\n",
    "        ensemble_predictions[name][img_id] = max(0.0, ensemble_predictions[name][img_id])\n",
    "    \n",
    "    clover = ensemble_predictions['Dry_Clover_g'][img_id]\n",
    "    dead = ensemble_predictions['Dry_Dead_g'][img_id]\n",
    "    green = ensemble_predictions['Dry_Green_g'][img_id]\n",
    "    gdm = ensemble_predictions['GDM_g'][img_id]\n",
    "    total = ensemble_predictions['Dry_Total_g'][img_id]\n",
    "    \n",
    "    # GDM = Green + Clover\n",
    "    gdm_calc = green + clover\n",
    "    adj_gdm = (gdm + gdm_calc) / 2\n",
    "    if gdm_calc > 0:\n",
    "        scale = adj_gdm / gdm_calc\n",
    "        ensemble_predictions['Dry_Green_g'][img_id] = green * scale\n",
    "        ensemble_predictions['Dry_Clover_g'][img_id] = clover * scale\n",
    "    ensemble_predictions['GDM_g'][img_id] = adj_gdm\n",
    "    \n",
    "    # Total = GDM + Dead\n",
    "    total_calc = adj_gdm + dead\n",
    "    adj_total = (total + total_calc) / 2\n",
    "    if adj_total > adj_gdm:\n",
    "        ensemble_predictions['Dry_Dead_g'][img_id] = adj_total - adj_gdm\n",
    "    else:\n",
    "        ensemble_predictions['Dry_Dead_g'][img_id] = 0.0\n",
    "        adj_total = adj_gdm\n",
    "    ensemble_predictions['Dry_Total_g'][img_id] = adj_total\n",
    "\n",
    "print(\"\\nPredictions summary:\")\n",
    "for name in TARGET_NAMES:\n",
    "    vals = list(ensemble_predictions[name].values())\n",
    "    print(f\"  {name:<15} mean: {np.mean(vals):>8.2f}  min: {np.min(vals):>8.2f}  max: {np.max(vals):>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "submission_rows = []\n",
    "for img_id in image_ids_order:\n",
    "    for name in TARGET_NAMES:\n",
    "        submission_rows.append({\n",
    "            'sample_id': f\"{img_id}__{name}\",\n",
    "            'target': ensemble_predictions[name][img_id]\n",
    "        })\n",
    "\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission created!\")\n",
    "print(submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
