{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - V7: Foundation Model Ensemble\n",
    "\n",
    "This notebook generates predictions using foundation model backbones (SigLIP or DINOv2):\n",
    "- **Backbone**: Vision Transformer with pretrained foundation weights\n",
    "- **Training**: Differential learning rates, warmup, gradient clipping\n",
    "- **Ensemble**: 5-fold cross-validation\n",
    "\n",
    "## Setup Instructions\n",
    "1. Add the model dataset\n",
    "2. Add the competition data\n",
    "3. **Set Internet to OFF** (required for submission)\n",
    "4. Run all cells to generate submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import gc\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"timm version: {timm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which model to use\n",
    "MODEL_TYPE = 'siglip'  # Options: 'siglip', 'dinov2'\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    'siglip': {\n",
    "        'backbone': 'vit_base_patch16_siglip_384',\n",
    "        'image_size': 384,\n",
    "        'features': 768,\n",
    "        'checkpoint_base': '/kaggle/input/image2biomass-siglip-v2/pytorch/default/1/checkpoints_siglip'\n",
    "    },\n",
    "    'dinov2': {\n",
    "        'backbone': 'vit_base_patch14_dinov2',\n",
    "        'image_size': 518,\n",
    "        'features': 768,\n",
    "        'checkpoint_base': '/kaggle/input/image2biomass-dinov2-v2/pytorch/default/1/checkpoints_dinov2'\n",
    "    }\n",
    "}\n",
    "\n",
    "CONFIG = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Paths\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Target names\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")\n",
    "print(f\"Backbone: {CONFIG['backbone']}\")\n",
    "print(f\"Image size: {CONFIG['image_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (Foundation Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoundationModelRegressor(nn.Module):\n",
    "    \"\"\"Multi-task regressor using foundation model backbone.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone_name, num_features=768, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.target_names = TARGET_NAMES\n",
    "\n",
    "        # Load backbone - handle DINOv2 differently\n",
    "        if 'dinov2' in backbone_name:\n",
    "            self.backbone = timm.create_model(\n",
    "                backbone_name,\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "            )\n",
    "        else:\n",
    "            self.backbone = timm.create_model(\n",
    "                backbone_name,\n",
    "                pretrained=False,\n",
    "                num_classes=0,\n",
    "                global_pool='avg'\n",
    "            )\n",
    "\n",
    "        # Regression heads\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for name in self.target_names:\n",
    "            self.heads[name] = nn.Sequential(\n",
    "                nn.Linear(num_features, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(256, 64),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(64, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return {name: self.heads[name](features).squeeze(-1) for name in self.target_names}\n",
    "\n",
    "print(\"Foundation model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    \"\"\"Test dataset for biomass prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_filename = Path(self.image_paths[image_id]).name\n",
    "        image_path = self.img_dir / image_filename\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return {'image': image, 'image_id': image_id}\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get Target Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data to compute normalization statistics\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "\n",
    "# Pivot to wide format\n",
    "train_wide = train_df.pivot_table(\n",
    "    index='image_id',\n",
    "    columns='target_name',\n",
    "    values='target',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Compute statistics\n",
    "target_stats = {}\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = train_wide[target_name].values\n",
    "    target_stats[target_name] = {\n",
    "        'mean': float(np.mean(values)),\n",
    "        'std': float(np.std(values)) + 1e-8\n",
    "    }\n",
    "\n",
    "print(\"Target normalization statistics:\")\n",
    "for target_name, stats in target_stats.items():\n",
    "    print(f\"  {target_name:<20} mean: {stats['mean']:>8.2f}  std: {stats['std']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation transforms\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = BiomassTestDataset(\n",
    "    csv_path=TEST_CSV,\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Models and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_predictions(pred_dict, target_stats):\n",
    "    \"\"\"Denormalize predictions back to original scale.\"\"\"\n",
    "    denormalized = {}\n",
    "    for target_name, value in pred_dict.items():\n",
    "        stats = target_stats[target_name]\n",
    "        denormalized[target_name] = (value * stats['std']) + stats['mean']\n",
    "    return denormalized\n",
    "\n",
    "# Generate predictions from each fold\n",
    "print(f\"Loading {N_FOLDS} fold models and generating predictions...\")\n",
    "all_fold_predictions = []\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    checkpoint_path = Path(CONFIG['checkpoint_base']) / f'fold_{fold_idx}' / 'best_model.pth'\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}:\")\n",
    "    print(f\"  Loading from: {checkpoint_path}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = FoundationModelRegressor(\n",
    "        backbone_name=CONFIG['backbone'],\n",
    "        num_features=CONFIG['features'],\n",
    "        dropout=0.3\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"  Val Loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    fold_preds = {}\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Fold {fold_idx + 1}\"):\n",
    "            images = batch['image'].to(DEVICE)\n",
    "            image_ids = batch['image_id']\n",
    "            \n",
    "            pred = model(images)\n",
    "            \n",
    "            for i, image_id in enumerate(image_ids):\n",
    "                pred_dict = {\n",
    "                    target_name: pred[target_name][i].cpu().item()\n",
    "                    for target_name in TARGET_NAMES\n",
    "                }\n",
    "                pred_dict = denormalize_predictions(pred_dict, target_stats)\n",
    "                fold_preds[image_id] = pred_dict\n",
    "    \n",
    "    all_fold_predictions.append(fold_preds)\n",
    "    print(f\"  Generated {len(fold_preds)} predictions\")\n",
    "    \n",
    "    # Free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nAll {N_FOLDS} folds processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ensemble and Apply Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_constraint(predictions, method='average'):\n",
    "    \"\"\"Enforce constraint: Dry_Total = Dry_Clover + Dry_Dead + Dry_Green\"\"\"\n",
    "    enforced = {}\n",
    "    \n",
    "    for image_id, pred_dict in predictions.items():\n",
    "        pred = pred_dict.copy()\n",
    "        \n",
    "        clover = pred['Dry_Clover_g']\n",
    "        dead = pred['Dry_Dead_g']\n",
    "        green = pred['Dry_Green_g']\n",
    "        total = pred['Dry_Total_g']\n",
    "        \n",
    "        component_sum = clover + dead + green\n",
    "        \n",
    "        if method == 'average':\n",
    "            new_total = (total + component_sum) / 2\n",
    "            \n",
    "            if component_sum > 0:\n",
    "                scale = new_total / component_sum\n",
    "                pred['Dry_Clover_g'] = max(0, clover * scale)\n",
    "                pred['Dry_Dead_g'] = max(0, dead * scale)\n",
    "                pred['Dry_Green_g'] = max(0, green * scale)\n",
    "                pred['Dry_Total_g'] = new_total\n",
    "            else:\n",
    "                pred['Dry_Total_g'] = max(0, total)\n",
    "        \n",
    "        # Ensure non-negative\n",
    "        for key in pred:\n",
    "            pred[key] = max(0, pred[key])\n",
    "        \n",
    "        enforced[image_id] = pred\n",
    "    \n",
    "    return enforced\n",
    "\n",
    "# Average predictions across folds\n",
    "print(\"Averaging predictions across folds...\")\n",
    "ensemble_predictions = {}\n",
    "\n",
    "all_image_ids = list(all_fold_predictions[0].keys())\n",
    "\n",
    "for image_id in all_image_ids:\n",
    "    ensemble_pred = {}\n",
    "    for target_name in TARGET_NAMES:\n",
    "        fold_values = [fold_preds[image_id][target_name] for fold_preds in all_fold_predictions]\n",
    "        ensemble_pred[target_name] = np.mean(fold_values)\n",
    "    ensemble_predictions[image_id] = ensemble_pred\n",
    "\n",
    "print(f\"Generated ensemble predictions for {len(ensemble_predictions)} images\")\n",
    "\n",
    "# Apply constraint enforcement\n",
    "print(\"\\nApplying constraint enforcement...\")\n",
    "ensemble_predictions = enforce_constraint(ensemble_predictions, method='average')\n",
    "\n",
    "# Check constraint violations\n",
    "violations = []\n",
    "for image_id, pred in ensemble_predictions.items():\n",
    "    total = pred['Dry_Total_g']\n",
    "    component_sum = pred['Dry_Clover_g'] + pred['Dry_Dead_g'] + pred['Dry_Green_g']\n",
    "    violation = abs(total - component_sum)\n",
    "    violations.append(violation)\n",
    "\n",
    "print(f\"Constraint violations:\")\n",
    "print(f\"  Mean: {np.mean(violations):.6f}g\")\n",
    "print(f\"  Max: {np.max(violations):.6f}g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.csv to get correct sample_id ordering\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Create submission rows\n",
    "submission_rows = []\n",
    "for _, row in test_df.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    image_id = sample_id.split('__')[0]\n",
    "    target_name = row['target_name']\n",
    "    \n",
    "    pred_value = ensemble_predictions[image_id][target_name]\n",
    "    \n",
    "    submission_rows.append({\n",
    "        'sample_id': sample_id,\n",
    "        'target': pred_value\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created!\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(submission_df['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEnsemble predictions by target:\")\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = [pred[target_name] for pred in ensemble_predictions.values()]\n",
    "    print(f\"  {target_name:<20} mean: {np.mean(values):>8.2f}  \"\n",
    "          f\"min: {np.min(values):>8.2f}  max: {np.max(values):>8.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Model: {MODEL_TYPE.upper()} Foundation Model\")\n",
    "print(f\"Backbone: {CONFIG['backbone']}\")\n",
    "print(f\"Image size: {CONFIG['image_size']}\")\n",
    "print(f\"Ensemble: {N_FOLDS}-fold\")\n",
    "print(\"Submission file ready: submission.csv\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
