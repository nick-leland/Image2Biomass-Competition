{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIRO Image2Biomass - V4: 5-Fold EfficientNetV2-M Ensemble (MSE Loss)\n",
    "\n",
    "This notebook generates predictions using a **5-fold ensemble** of EfficientNetV2-M models:\n",
    "- **Loss function**: MSE (better aligned with R^2 metric)\n",
    "- Mean validation loss: 1.95 +/- 0.36 (MSE)\n",
    "- Image size: 512x512\n",
    "- Conservative augmentation during training\n",
    "- Ensemble reduces variance and improves robustness\n",
    "\n",
    "## Setup Instructions\n",
    "1. Add the model dataset (image2biomass-efficientnetv2-kfold-mse)\n",
    "2. Add the competition data\n",
    "3. **Set Internet to OFF** (required for submission)\n",
    "4. Run all cells to generate submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"timm version: {timm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "TEST_CSV = '/kaggle/input/csiro-biomass/test.csv'\n",
    "TEST_IMG_DIR = '/kaggle/input/csiro-biomass/test'\n",
    "TRAIN_CSV = '/kaggle/input/csiro-biomass/train.csv'\n",
    "TRAIN_IMG_DIR = '/kaggle/input/csiro-biomass/train'\n",
    "\n",
    "# Model checkpoints path\n",
    "CHECKPOINT_BASE = '/kaggle/input/image2biomass-efficientnetv2-kfold-mse/pytorch/default/1/checkpoints_kfold_mse_kaggle'\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Target names\n",
    "TARGET_NAMES = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "\n",
    "# Model config (MSE loss version)\n",
    "CONFIG = {\n",
    "    'backbone': 'tf_efficientnetv2_m',\n",
    "    'image_size': 512,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 0,  # Set to 0 for Kaggle\n",
    "    'dropout': 0.5,\n",
    "    'head_hidden_dim': 512\n",
    "}\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture (EfficientNetV2-M with timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskEfficientNet(nn.Module):\n",
    "    \"\"\"Multi-task EfficientNetV2-M for biomass prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='tf_efficientnetv2_m', num_targets=5, \n",
    "                 dropout=0.5, head_hidden_dim=512, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load EfficientNetV2-M backbone from timm\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classification head\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        \n",
    "        # Get backbone output features\n",
    "        backbone_features = self.backbone.num_features\n",
    "        \n",
    "        # Create prediction heads for each target\n",
    "        self.heads = nn.ModuleDict({\n",
    "            target_name: self._make_head(backbone_features, head_hidden_dim, dropout)\n",
    "            for target_name in TARGET_NAMES\n",
    "        })\n",
    "    \n",
    "    def _make_head(self, in_features, hidden_dim, dropout):\n",
    "        \"\"\"Create a prediction head.\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        outputs = {\n",
    "            target_name: self.heads[target_name](features).squeeze(-1)\n",
    "            for target_name in TARGET_NAMES\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "print(\"Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Class (with Albumentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiomassTestDataset(Dataset):\n",
    "    \"\"\"Test dataset for biomass prediction using Albumentations.\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, img_dir, transform=None, target_stats=None):\n",
    "        self.csv_path = csv_path\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "        self.target_stats = target_stats\n",
    "        \n",
    "        # Load CSV\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['image_id'] = self.df['sample_id'].str.split('__').str[0]\n",
    "        self.image_ids = self.df['image_id'].unique()\n",
    "        self.image_paths = self.df.groupby('image_id')['image_path'].first().to_dict()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_filename = Path(self.image_paths[image_id]).name\n",
    "        image_path = self.img_dir / image_filename\n",
    "        \n",
    "        # Load image and convert to numpy array for Albumentations\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Apply Albumentations transforms\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "print(\"Dataset class defined (Albumentations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get Target Statistics from Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data to compute normalization statistics\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "train_df['image_id'] = train_df['sample_id'].str.split('__').str[0]\n",
    "\n",
    "# Pivot to wide format\n",
    "train_wide = train_df.pivot_table(\n",
    "    index='image_id',\n",
    "    columns='target_name',\n",
    "    values='target',\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Compute statistics\n",
    "target_stats = {}\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = train_wide[target_name].values\n",
    "    target_stats[target_name] = {\n",
    "        'mean': float(np.mean(values)),\n",
    "        'std': float(np.std(values)) + 1e-8\n",
    "    }\n",
    "\n",
    "print(\"Target normalization statistics:\")\n",
    "for target_name, stats in target_stats.items():\n",
    "    print(f\"  {target_name:<20} mean: {stats['mean']:>8.2f}  std: {stats['std']:>8.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Test Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation transforms using Albumentations (same as training)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CONFIG['image_size'], CONFIG['image_size']),\n",
    "    A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = BiomassTestDataset(\n",
    "    csv_path=TEST_CSV,\n",
    "    img_dir=TEST_IMG_DIR,\n",
    "    transform=val_transform,\n",
    "    target_stats=target_stats\n",
    ")\n",
    "\n",
    "# Create dataloader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG['num_workers'],\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load All Fold Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 5 fold models\n",
    "fold_models = []\n",
    "\n",
    "print(f\"Loading {N_FOLDS} fold models (MSE loss)...\")\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    checkpoint_path = Path(CHECKPOINT_BASE) / f'fold_{fold_idx}' / 'best_model.pth'\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}:\")\n",
    "    print(f\"  Loading from: {checkpoint_path}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiTaskEfficientNet(\n",
    "        backbone=CONFIG['backbone'],\n",
    "        num_targets=len(TARGET_NAMES),\n",
    "        dropout=CONFIG['dropout'],\n",
    "        head_hidden_dim=CONFIG['head_hidden_dim'],\n",
    "        pretrained=False  # Weights from checkpoint\n",
    "    )\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "    print(f\"  Val Loss (MSE): {checkpoint['best_val_loss']:.4f}\")\n",
    "    \n",
    "    fold_models.append(model)\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(fold_models)} models\")\n",
    "\n",
    "n_params = sum(p.numel() for p in fold_models[0].parameters())\n",
    "print(f\"Parameters per model: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Ensemble Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_predictions(pred_dict, target_stats):\n",
    "    \"\"\"Denormalize predictions back to original scale.\"\"\"\n",
    "    denormalized = {}\n",
    "    for target_name, value in pred_dict.items():\n",
    "        stats = target_stats[target_name]\n",
    "        denormalized[target_name] = (value * stats['std']) + stats['mean']\n",
    "    return denormalized\n",
    "\n",
    "def enforce_constraint(predictions, method='average'):\n",
    "    \"\"\"Enforce constraint: Dry_Total = Dry_Clover + Dry_Dead + Dry_Green\"\"\"\n",
    "    enforced = {}\n",
    "    \n",
    "    for image_id, pred_dict in predictions.items():\n",
    "        pred = pred_dict.copy()\n",
    "        \n",
    "        clover = pred['Dry_Clover_g']\n",
    "        dead = pred['Dry_Dead_g']\n",
    "        green = pred['Dry_Green_g']\n",
    "        total = pred['Dry_Total_g']\n",
    "        \n",
    "        component_sum = clover + dead + green\n",
    "        \n",
    "        if method == 'average':\n",
    "            # Average the predicted total and sum of components\n",
    "            new_total = (total + component_sum) / 2\n",
    "            \n",
    "            # Distribute discrepancy proportionally\n",
    "            if component_sum > 0:\n",
    "                scale = new_total / component_sum\n",
    "                pred['Dry_Clover_g'] = clover * scale\n",
    "                pred['Dry_Dead_g'] = dead * scale\n",
    "                pred['Dry_Green_g'] = green * scale\n",
    "                pred['Dry_Total_g'] = new_total\n",
    "            else:\n",
    "                pred['Dry_Total_g'] = 0.0\n",
    "        \n",
    "        enforced[image_id] = pred\n",
    "    \n",
    "    return enforced\n",
    "\n",
    "# Generate predictions from each fold\n",
    "print(\"Generating predictions from ensemble...\")\n",
    "all_fold_predictions = []\n",
    "\n",
    "for fold_idx, model in enumerate(fold_models):\n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}:\")\n",
    "    fold_preds = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Fold {fold_idx + 1}\"):\n",
    "            images = batch['image'].to(DEVICE)\n",
    "            image_ids = batch['image_id']\n",
    "            \n",
    "            # Get predictions\n",
    "            pred = model(images)\n",
    "            \n",
    "            # Store predictions for each image\n",
    "            for i, image_id in enumerate(image_ids):\n",
    "                pred_dict = {\n",
    "                    target_name: pred[target_name][i].cpu().item()\n",
    "                    for target_name in TARGET_NAMES\n",
    "                }\n",
    "                # Denormalize\n",
    "                pred_dict = denormalize_predictions(pred_dict, target_stats)\n",
    "                fold_preds[image_id] = pred_dict\n",
    "    \n",
    "    all_fold_predictions.append(fold_preds)\n",
    "    print(f\"  Generated {len(fold_preds)} predictions\")\n",
    "\n",
    "# Average predictions across folds\n",
    "print(\"\\nAveraging predictions across folds...\")\n",
    "ensemble_predictions = {}\n",
    "\n",
    "# Get all image IDs\n",
    "all_image_ids = list(all_fold_predictions[0].keys())\n",
    "\n",
    "for image_id in all_image_ids:\n",
    "    ensemble_pred = {}\n",
    "    for target_name in TARGET_NAMES:\n",
    "        # Collect predictions from all folds\n",
    "        fold_values = [fold_preds[image_id][target_name] for fold_preds in all_fold_predictions]\n",
    "        # Average them\n",
    "        ensemble_pred[target_name] = np.mean(fold_values)\n",
    "    ensemble_predictions[image_id] = ensemble_pred\n",
    "\n",
    "print(f\"Generated ensemble predictions for {len(ensemble_predictions)} images\")\n",
    "\n",
    "# Apply constraint enforcement\n",
    "print(\"\\nApplying constraint enforcement...\")\n",
    "ensemble_predictions = enforce_constraint(ensemble_predictions, method='average')\n",
    "\n",
    "# Check constraint violations\n",
    "violations = []\n",
    "for image_id, pred in ensemble_predictions.items():\n",
    "    total = pred['Dry_Total_g']\n",
    "    component_sum = pred['Dry_Clover_g'] + pred['Dry_Dead_g'] + pred['Dry_Green_g']\n",
    "    violation = abs(total - component_sum)\n",
    "    violations.append(violation)\n",
    "\n",
    "print(f\"Constraint violations:\")\n",
    "print(f\"  Mean: {np.mean(violations):.6f}g\")\n",
    "print(f\"  Max: {np.max(violations):.6f}g\")\n",
    "print(f\"  All exact: {all(v < 1e-6 for v in violations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test.csv to get correct sample_id ordering\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Create submission rows\n",
    "submission_rows = []\n",
    "for _, row in test_df.iterrows():\n",
    "    sample_id = row['sample_id']\n",
    "    image_id = sample_id.split('__')[0]\n",
    "    target_name = row['target_name']\n",
    "    \n",
    "    # Get prediction\n",
    "    pred_value = ensemble_predictions[image_id][target_name]\n",
    "    \n",
    "    submission_rows.append({\n",
    "        'sample_id': sample_id,\n",
    "        'target': pred_value\n",
    "    })\n",
    "\n",
    "# Create DataFrame and save\n",
    "submission_df = pd.DataFrame(submission_rows)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created!\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(submission_df['target'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Display Predictions Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEnsemble predictions by target:\")\n",
    "for target_name in TARGET_NAMES:\n",
    "    values = [pred[target_name] for pred in ensemble_predictions.values()]\n",
    "    print(f\"  {target_name:<20} mean: {np.mean(values):>8.2f}  \"\n",
    "          f\"min: {np.min(values):>8.2f}  max: {np.max(values):>8.2f}\")\n",
    "\n",
    "# Show individual fold predictions for comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Individual fold predictions (for first image):\")\n",
    "print(\"=\"*70)\n",
    "first_image_id = list(ensemble_predictions.keys())[0]\n",
    "for target_name in TARGET_NAMES:\n",
    "    print(f\"\\n{target_name}:\")\n",
    "    for fold_idx in range(N_FOLDS):\n",
    "        pred = all_fold_predictions[fold_idx][first_image_id][target_name]\n",
    "        print(f\"  Fold {fold_idx + 1}: {pred:>8.2f}\")\n",
    "    ensemble_pred = ensemble_predictions[first_image_id][target_name]\n",
    "    print(f\"  Ensemble: {ensemble_pred:>8.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Submission file ready: submission.csv\")\n",
    "print(\"5-Fold Ensemble with EfficientNetV2-M (MSE Loss)\")\n",
    "print(\"Ready to submit with Internet OFF!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
